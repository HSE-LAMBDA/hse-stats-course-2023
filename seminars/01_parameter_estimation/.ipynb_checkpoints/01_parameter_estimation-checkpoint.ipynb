{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Примерный) план семинаров\n",
    "\n",
    "\n",
    "    1. Вероятность & формула Байеса, MLE & MAP.\n",
    "    2. Робастные оценки\n",
    "    3. Неравенство Рао-Крамера, предельные частотные интервальные оценки (теорема Вальда и Вилкса).\n",
    "    4. Построение/интервал Неймана, мешающие параметры, байесовские интервальные оценки.\n",
    "\n",
    "\n",
    "    5. Обобщённые линейные модели (GLM) и их оптимизация.\n",
    "    6. Метрики качества, регуляризация GLM, ROC-AUC и непараметрические тесты.\n",
    "    7. Дисперсионный анализ, сравнение различных тестов.\n",
    "    8. Множественное тестирование на примере изучения МРТ-сигнала снятого с мозга.\n",
    "    9. Кросс-валидация, смещённость кросс-валидационных оценок, способы коррекции смещённости.\n",
    "\n",
    "\n",
    "    10. Оптимизация прямой и обратной KL-дивергенции и расстояния Вассершетейна на игрушечной проблеме.\n",
    "    11. Непараметрическая регрессия, предсказание доверительных интервалов с помощью непараметрической регрессии, оценки риска.\n",
    "    12. Различные вариационные оценки и неравенства для дивергенций и расстояний.\n",
    "    13. ELBO, вариационные оценки.\n",
    "\n",
    "\n",
    "    14. Теоретические задачки на случайное блуждание, оценки стоимости опциона в дискретной биномиальной модели.\n",
    "    15. Применение MCMC для оценки стоимости опциона.\n",
    "    16. Гамильтоново Монте-Карло.\n",
    "\n",
    "\n",
    "    17. GPyTorch, построение модели гауссовых процессов с цилиндрическим ядром, экзотические ядра и их связь с топологией.\n",
    "    18. Построение модели гауссовых процессов с нестационарными ядрами, ядерная гребневая регрессия.\n",
    "    19. Дизайн экспериментов с помощью GP, построение суррогатных моделей гауссовскими процессами. \n",
    "    20. Продолжение дизайна экспериментов, выбор ковариаций для гауссовых процессов, kernel trick. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 1: Вероятность & Байес, MLE & MAP\n",
    "\n",
    "## Минутка теории\n",
    "\n",
    "### Условная вероятность\n",
    "\n",
    "Пусть есть два события $A$ и $B$. Тогда условная вероятность вводится следующим образом (аксиоматически):\n",
    "\n",
    "$$p(A|B) = \\frac{p(A, B)}{p(B)}$$\n",
    "\n",
    "Это есть вероятность что наступит событие $A$, если событие $B$ уже наступило.\n",
    "\n",
    "Если $p(A|B)=p(A)$, то говорят что событие $A$ не зависит от $B$.\n",
    "\n",
    "\n",
    "### Формула Байеса\n",
    "\n",
    "Формула Байеса выводится из формулы условной вероятности в два щелчка:\n",
    "\n",
    "$$p(A|B) = \\frac{p(A, B)}{p(B)} = \\frac{p(A) p(B|A)}{p(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 1. Разминка для мозгов\n",
    "\n",
    "В семье два ребёнка. Каждый ребёнок мальчик или девочка с равной вероятностью 50%.\n",
    "\n",
    "Тогда полное пространство событий описывается четырьмя событиями.\n",
    "\n",
    "![](full_space_boy_girl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Если известно только что в семье два ребёнка, то какая вероятность, что оба мальчики?\n",
    "\n",
    "$$p(bb) = \\frac{1}{2} \\frac{1}{2} = \\frac{1}{4}$$\n",
    "\n",
    "![](two_boys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Если известно что один из детей мальчик, то какая вероятность, что другой ребёнок тоже мальчик?\n",
    "\n",
    "$$p(bb | b) = \\frac{p(bb) p(b | bb)}{p(b)} = \\frac{\\frac{1}{4} \\cdot 1}{\\frac{3}{4}} = \\frac{1}{3}$$\n",
    "\n",
    "$p(b)$ - вероятность что хотя бы один мальчик.\n",
    "\n",
    "![](one_is_a_boy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Если известно что в семье старший ребёнок мальчик, то какая вероятность, что младший тоже мальчик?\n",
    "\n",
    "$$p(bb | b_{old} ) = \\frac{p(bb_{old}) p(b_{old} | bb)}{p(b_{old})} = \\frac{\\frac{1}{4} \\cdot 1}{\\frac{1}{2}} = \\frac{1}{2}$$\n",
    "\n",
    "Разница между предыдущим примером и этим в том что мы определили порядок, что сильно влияет на вероятности потому что явным образом определяет столбец (см. картинку).\n",
    "\n",
    "![](old_is_boy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Если известно что в семье один ребёнок мужского пола и родился во вторник, то какая вероятность, что второй тоже мальчик?\n",
    "\n",
    "$$p(bb | b_{tuesday} ) = \\frac{p(bb) p(b_{tuesday} | bb)}{p(b_{tuesday})}$$\n",
    "\n",
    "Вероятность что хотя бы один мальчик родился во вторник это (1 - вороятность что ни один не родился во вторник).\n",
    "\n",
    "$$p(b_{tuesday} | bb) = \\left(1 - \\left(1 - \\frac{1}{7}\\right)^2\\right) = \\frac{13}{49}$$\n",
    "\n",
    "Это равно синей площади на графике ниже.\n",
    "\n",
    "$p(b_{tuesday})$ посчитаем по картинке:\n",
    "\n",
    "$$b_{tuesday} = \\frac{1}{4} \\frac{1}{7} + \\frac{1}{4} \\frac{1}{7} + \\frac{1}{4} p(b_{tuesday} | bb) = \\frac{27}{196}$$\n",
    "\n",
    "Тогда,\n",
    "\n",
    "$$p(bb | b_{tuesday} ) = \\frac{\\frac{1}{4} \\frac{13}{49}}{\\frac{27}{196}} = \\frac{13}{27} \\approx 0.481$$\n",
    "\n",
    "![](tuesday_boy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мы проходили мимо дома семьи и увидели в окне мальчика, какая вероятность что второй ребёнок тоже мальчик?\n",
    "\n",
    "$$p(b) = \\frac{1}{2}$$\n",
    "\n",
    "![](one_is_a_boy_obs.png)\n",
    "\n",
    "Важный момент! \n",
    "\n",
    "Априорное знание что один ребёнок мальчик не тоже самое что пронаблюдать что один из детей мальчик, потому что во втором случае мы фиксируем порядок детей (\"ребёнок которого пронаблюдали первым\" и \"другой ребёнок\"). То есть в этом случае ситуация похожа на пример со \"старшим ребёнком\". Это пример того что поряд может быть зафиксирован очень разными способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 2. Теперь мы генетики.\n",
    "\n",
    "По Дарвину приспособленность является центральной концепцией теории эволюции. \n",
    "\n",
    "Давайте определим \"относительную приспособленность\" как среднее количество выживших представителей одного генотипа к среднему количеству выживших представителей другого генотипа. \n",
    "\n",
    "Eyre-Walker (2006) ( https://www.ncbi.nlm.nih.gov/pubmed/16547091 ) предположили, что функция преспособленности принадлежит семейству гамма-распределений случайных величин. \n",
    "\n",
    "Они проверили своё предположение зафитировав количество мутаций ведущих к генерации в организме человека \"вредных\" аминокислот (которые ухудшают наше состояние) и получили следующие оценки на параметры гамма-распределения:\n",
    " \n",
    "$$\\hat{\\alpha} = 0.23,~~~ \\hat{\\beta} = 5.35$$\n",
    "\n",
    "\n",
    "$$\\Gamma(x|\\alpha, \\beta) =  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x),$$\n",
    "\n",
    "где $\\Gamma(\\alpha)$ -- гамма-функция Эйлера (для целых $\\alpha$: $\\Gamma(k) = (\\alpha-1)!$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Каким вопросом зададимся мы?\n",
    "\n",
    "Если мы оцениваем параметры $\\alpha$ и $\\beta$ по случайной выборке, как сильно мы в среднем ошибаемся при размере выборки $N=100$? \n",
    "\n",
    "Т.е. нас интересуют две вещи: __дисперсии__ наших оценок и __корреляция__ оценок.\n",
    "\n",
    "Оценивать мы будем их по следующему алгоритму:\n",
    "\n",
    "  1. Сэмплируем выборку $X \\sim \\Gamma(x|\\hat{\\alpha}, \\hat{\\beta})$\n",
    "  2. Оцениваем $\\alpha_{fit}, \\beta_{fit}$ по выборке $X$\n",
    "  3. Сохраняем $\\alpha_{fit}, \\beta_{fit}$\n",
    "  4. Повторяем шаги 1-3 N раз.\n",
    "  5. Считаем дисперсии и корреляции $\\{\\alpha_{fit}, \\beta_{fit}\\}_{i=1}^{B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Метод оценки параметров максимума правдоподобия\n",
    "\n",
    "MLE(Метод оценки параметров максимума правдоподобия) основывается на максимизации вероятности пронаблюдать выборку.\n",
    "\n",
    "$$\\mathcal{L} = \\prod p(x_i|\\theta)$$\n",
    "\n",
    "Будем рассматривать MLE на примере задачи выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale == \\theta\n",
    "# в scipy все распределения определены с точность до смещения и масштаба\n",
    "x = np.linspace(0, 1, 100)\n",
    "alpha = 0.23\n",
    "beta = 5.35\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(gamma.pdf(x, scale=1 / beta, a=alpha))\n",
    "plt.title(\"Gamma distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "X = gamma.rvs(scale=1 / beta, a=alpha, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma.pdf(X[0], scale=1 / beta, a=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(gamma.pdf(X, scale=1 / beta, a=2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравнение правдоподобия чаще всего записывается с использованием логарифма вероятности, так как в этом случае произведение переходит в сумму, что сильно упрощает жизнь.\n",
    "\n",
    "$$\\mathcal{\\log L} = \\sum\\limits_i \\log p(x_i|\\theta)$$\n",
    "\n",
    "  * более хорошая численная стабильность(предотвращает overflow или underflow ошибки);\n",
    "  * более (относительно)точные градиенты;\n",
    "  * более гладкая задача(методы оптимизации лучше работают).\n",
    "\n",
    "Посмотрим на зависимость log-likelihood для нашей задачи с гамма-распределением.\n",
    "\n",
    "$$\\mathcal{L} = \\prod\\limits_i \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} {x_i}^{\\alpha-1} \\exp(-\\beta x_i)$$\n",
    "\n",
    "$$\\mathcal{\\log L} = \\sum\\limits_i \\left(  \\alpha \\log \\beta - \\log \\Gamma(\\alpha) - \\beta x_i + (\\alpha - 1) \\log x_i \\right)$$\n",
    "\n",
    "(Обратим внимание насколько простой стала зависимость от $x_i$:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0, 1, 200)\n",
    "likelihood = np.prod(\n",
    "    gamma.pdf(\n",
    "        X[:, np.newaxis], \n",
    "        scale=1 / beta, \n",
    "        a=a), \n",
    "        axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(a, likelihood)\n",
    "plt.title('Likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = np.sum(gamma.logpdf(X[:, np.newaxis], \n",
    "                        scale=1 / beta, \n",
    "                        a=a), \n",
    "            axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(a, log_likelihood)\n",
    "plt.title('Log-Likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE? MLE!\n",
    "\n",
    "В большинстве задач, если это не игровые примеры, найти явную форму для оценки параметров с помощью MLE очень сложно.\n",
    "\n",
    "$$\\theta^* = \\arg \\max \\log \\mathcal{L}(X, \\theta)$$\n",
    "\n",
    "Берём первую производную по каждому параметра распределения и приравниваем к нулю:\n",
    "\n",
    "$$ \\mathcal{\\log L}'(X, \\theta) = 0$$\n",
    "\n",
    "Для параметра $\\beta$ гамма-распределения:\n",
    "\n",
    "$$\\mathcal{\\log L}'_\\beta = \\sum\\limits_{i} \\frac{\\alpha}{\\beta} - x_i = 0$$\n",
    "\n",
    "$$\\beta = \\frac{\\alpha N}{\\sum\\limits_i x_i}$$\n",
    "\n",
    "Для параметра $\\alpha$:\n",
    "\n",
    "$$\\mathcal{\\log L}'_\\alpha = \\sum \\log \\beta + \\log x_i - \\psi(\\alpha) = 0$$\n",
    "\n",
    "$$\\alpha = \\log \\beta + \\frac{1}{N} \\sum \\log x_i$$\n",
    "\n",
    "\n",
    "Итого:\n",
    "\n",
    "$$\\alpha = \\log \\beta + \\frac{1}{N} \\sum \\log x_i$$\n",
    "\n",
    "$$\\beta = \\frac{\\alpha N}{\\sum\\limits_i x_i}$$\n",
    "\n",
    "Увы, точного решения нет. \n",
    "\n",
    "На хвала разработчикам open source software!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, _, beta = gamma.fit(X, floc=0.)\n",
    "beta = 1 / beta\n",
    "print(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пришло время эксперимента :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "boot = 1000\n",
    "alphas = []\n",
    "betas = []\n",
    "fitted_alphas = []\n",
    "fitted_betas = []\n",
    "for i in range(boot):\n",
    "    X = gamma.rvs(scale=1 / beta, a=alpha, size=100)\n",
    "    fit_alpha, _, fit_beta = gamma.fit(X, floc=0)\n",
    "    fitted_betas.append(1 / fit_beta)\n",
    "    fitted_alphas.append(fit_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(fitted_alphas, bins=100);\n",
    "alpha_q5, alhpa_q95 = np.percentile(fitted_alphas, [5, 95])\n",
    "plt.axvline(x=alpha, linewidth=3, color='r',linestyle='--')\n",
    "plt.axvline(x=np.mean(fitted_alphas), linewidth=3, color='b',linestyle='--')\n",
    "plt.axvline(x=alpha_q5, linewidth=3, color='g',linestyle='--')\n",
    "plt.axvline(x=alhpa_q95, linewidth=3, color='g',linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(fitted_alphas) - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(fitted_betas, bins=100);\n",
    "beta_q5, beta_q95 = np.percentile(fitted_betas, [5, 95])\n",
    "plt.axvline(x=beta, linewidth=3, color='r',linestyle='--')\n",
    "plt.axvline(x=np.mean(fitted_betas), linewidth=3, color='b',linestyle='--')\n",
    "plt.axvline(x=beta_q5, linewidth=3, color='g',linestyle='--')\n",
    "plt.axvline(x=beta_q95, linewidth=3, color='g',linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(fitted_betas) - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(fitted_alphas, fitted_betas)\n",
    "plt.axvline(x=alpha, linewidth=3, color='r',linestyle='--')\n",
    "plt.axvline(x=np.mean(fitted_alphas), linewidth=3, color='b', linestyle='--')\n",
    "plt.axhline(y=beta, linewidth=3, color='r',linestyle='--')\n",
    "plt.axhline(y=np.mean(fitted_betas), linewidth=3, color='b', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(fitted_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(fitted_alphas, fitted_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "pearsonr(fitted_alphas, fitted_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка смещения\n",
    "\n",
    "Как мы видим, MLE дал смещённую оценку. Это связано с тем что MLE даёт состоятельную, __асимптотически__ эффективную и __асимптотически__ нормальную оценку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "boot = 10000\n",
    "sizes = np.logspace(1, 3, 10).astype(int)\n",
    "\n",
    "bias_alphas = []\n",
    "bias_betas = []\n",
    "\n",
    "for size in sizes:\n",
    "    fitted_alphas = []\n",
    "    fitted_betas = []\n",
    "    for i in range(boot):\n",
    "        X = gamma.rvs(scale=1 / beta, a=alpha, size=size)\n",
    "        fit_alpha, _, fit_beta = gamma.fit(X, floc=0)\n",
    "        fitted_betas.append(1 / fit_beta)\n",
    "        fitted_alphas.append(fit_alpha)\n",
    "        \n",
    "    bias_alphas.append(np.mean(fitted_alphas) - alpha)\n",
    "    bias_betas.append(np.mean(fitted_betas) - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Bias of alpha estimate vs sample size\")\n",
    "plt.xlabel(\"1 / n\")\n",
    "plt.ylabel(\"Bias of beta estimate\")\n",
    "plt.plot(1 / sizes, bias_alphas);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Bias of alpha estimate vs sample size\")\n",
    "plt.xlabel(\"1 / n\")\n",
    "plt.ylabel(\"Bias of alpha estimate\")\n",
    "plt.plot(1 / sizes, bias_betas);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, $\\textrm{bias} \\sim \\frac{1}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Страшно ли это?\n",
    "\n",
    "Зависит от контекста, но в целом MLE-оценки являются $\\sqrt{n}$-состоятельной, то есть дисперсия оценки уменьшается значительно медленнее чем смещение, поэтому дисперсия практически всегда доминирует над наблюдаемым смещением (сравните ширину гистограммы и положение смещения на графиках выше).\n",
    "\n",
    "### Что делать если вы всё же хотите избавиться от смещения?\n",
    "\n",
    "1. Аналитически посчитать смещение: $$\\mathrm{bias} = E_{p(X | \\theta)} \\theta_{MLE} - \\theta$$\n",
    "\n",
    "\n",
    "#### Пример\n",
    "\n",
    "В качестве примера возьмём равномерное распределение: $x_i \\sim U[0, a]$. \n",
    "\n",
    "$$\\log p(x|a) = - \\sum_{x_i}\\log {a} = - n \\log a$$\n",
    "\n",
    "$$\\frac{d}{d a} \\log p(x|a) = -\\frac{n}{a}$$\n",
    "\n",
    "Так как производная монотонная (то есть лайклихуд достигает максимума на границе интервала), то MLE-оценка будет следующая:\n",
    "\n",
    "$$a_{MLE} = \\max_i{x_i}$$\n",
    "\n",
    "А теперь оценим смещение MLE оценки. Первым делом выпишем функцию распределения максимума:\n",
    "\n",
    "$$P(\\max_i{x_i} < T) = P(x_1 < T)P(x_2 < T)\\dots P(x_n < T) = \\left( \\frac{T}{a} \\right)^n$$\n",
    "\n",
    "Плотность распределения максимума: \n",
    "\n",
    "$$p(\\max_i{x_i}) = \\frac{d}{dT} P(\\max_i{x_i} < T) = n \\left( \\frac{T}{a} \\right)^{n-1} \\frac{1}{a} $$\n",
    "\n",
    "Аналитически считаем смещение\n",
    "\n",
    "$$E_{p(\\max_i{x_i}|a)} a_{MLE} = \\int n  a_{MLE} \\left( \\frac{a_{MLE}}{a} \\right)^{n-1} \\frac{1}{a} da_{MLE} = \\frac{n}{n+1} a $$\n",
    "\n",
    "То есть чтобы пофиксить байес нужно лишь домножить $\\frac{n+1}{n} a_{MLE}$.\n",
    "\n",
    "2. Если аналитически посчитать смещение сложноо, то можно воспользоваться бутстрапом. Если лайклихуд гладкий, то очень хорошо работает jacknife-бутстрап. \n",
    "\n",
    "3. Более экзотические методы: коррекция Cox-Snell: \n",
    "\n",
    "https://www.jstor.org/stable/2984505\n",
    "\n",
    "https://journal.r-project.org/archive/2017/RJ-2017-055/RJ-2017-055.pdf\n",
    "\n",
    "    \n",
    "(1) требует повышенных ручных вычислений, (2) требует вычислительных более серьёзных мощностей, (3) вычислительно лёгкая и не требует аналитических выкладок, но избавляется только от $(1 / n)$-смещения, для более тонкой коррекции ($1/n^2$ и т.д.) требуется итеративное применение процедуры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Метод моментов\n",
    "\n",
    "С гамма-распределением нам повезло что в `scipy` есть метод для фитирования правдоподобия. А если бы его не было?.. Например, если мы сами придумали новое распределение для которого непосредственно оптимизировать лайклихуд очень сложно.\n",
    "\n",
    "Если мы хотим определить параметры некоторого распределения $p(x | \\theta)$, то давайте приравняем моменты данного распределения к моментам подсчитанным по данным($X = \\{ x_1, \\dots, x_n \\}$) и решим уравнения относительно данных моментов.\n",
    "\n",
    "Моменты по данным считаются следующим образом:\n",
    "\n",
    "$$\\hat{\\mu}_k = \\frac{1}{n} \\sum\\limits_{i=1}^{n} x_i^k$$\n",
    "\n",
    "Моменты распределений:\n",
    "\n",
    "$$\\mu_k = \\sum\\limits_{i=1}^{n} x^k p(x | \\theta) dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "# scale == \\theta\n",
    "# в scipy все распределения определены с точность до смещения и масштаба\n",
    "x = np.linspace(0, 1, 100)\n",
    "alpha = 0.23\n",
    "beta = 5.35\n",
    "plt.plot(gamma.pdf(x, scale=1 / beta, a=alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее и дисперсия гамма-распределения:\n",
    "\n",
    "$$\\mu(\\Gamma) = \\frac{\\alpha}{\\beta}$$\n",
    "\n",
    "$$Var(\\Gamma) = \\frac{\\alpha}{\\beta^2}$$\n",
    "\n",
    "Применяем метод моментов:\n",
    "\n",
    "$$\\hat{\\mu}_1 = \\frac{\\alpha}{\\beta}$$\n",
    "\n",
    "$$\\hat{\\mu}_2 = \\frac{\\alpha^2}{\\beta^2} + \\frac{\\alpha}{\\beta^2}$$\n",
    "\n",
    "Решая систему уравнений получаем:\n",
    "\n",
    "$$\\beta = \\frac{\\hat{\\mu}_1}{\\hat{\\mu}_2 - \\hat{\\mu}_1^2}$$\n",
    "\n",
    "$$\\alpha = \\hat{\\mu}_1 \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "boot = 10000\n",
    "fitted_alphas = []\n",
    "fitted_betas = []\n",
    "for i in range(boot):\n",
    "    X = gamma.rvs(scale=1 / beta, a=alpha, size=1000)\n",
    "    mu_1 = X.mean()\n",
    "    mu_2 = np.square(X).mean()\n",
    "    fitted_betas.append(\n",
    "        mu_1 / (mu_2 - mu_1**2)\n",
    "    )\n",
    "    \n",
    "    fitted_alphas.append(\n",
    "        mu_1**2 / (mu_2 - mu_1**2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fitted_alphas, bins=100);\n",
    "alpha_q5, alhpa_q95 = np.percentile(fitted_alphas, [5, 95])\n",
    "plt.axvline(x=alpha, linewidth=3, color='r',linestyle='--')\n",
    "plt.axvline(x=alpha_q5, linewidth=3, color='g',linestyle='--')\n",
    "plt.axvline(x=alhpa_q95, linewidth=3, color='g',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(fitted_alphas) - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fitted_betas, bins=100);\n",
    "beta_q5, beta_q95 = np.percentile(fitted_betas, [5, 95])\n",
    "plt.axvline(x=beta, linewidth=3, color='r',linestyle='--')\n",
    "plt.axvline(x=beta_q5, linewidth=3, color='g',linestyle='--')\n",
    "plt.axvline(x=beta_q95, linewidth=3, color='g',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(fitted_betas) - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(fitted_alphas, fitted_betas)\n",
    "plt.axvline(x=alpha, linewidth=3, color='r',linestyle='--')\n",
    "plt.axvline(x=np.mean(fitted_alphas), linewidth=3, color='b', linestyle='--')\n",
    "plt.axhline(y=beta, linewidth=3, color='r',linestyle='--')\n",
    "plt.axhline(y=np.mean(fitted_betas), linewidth=3, color='b', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "np.corrcoef(fitted_alphas, fitted_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(fitted_alphas, fitted_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценим смещение для метода моментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "boot = 10000\n",
    "sizes = np.logspace(1, 3, 10).astype(int)\n",
    "\n",
    "bias_alphas = []\n",
    "bias_betas = []\n",
    "\n",
    "for size in sizes:\n",
    "    fitted_alphas = []\n",
    "    fitted_betas = []\n",
    "    for i in range(boot):\n",
    "        X = gamma.rvs(scale=1 / beta, a=alpha, size=size)\n",
    "        mu_1 = X.mean()\n",
    "        mu_2 = np.square(X).mean()\n",
    "        fitted_betas.append(\n",
    "            mu_1 / (mu_2 - mu_1**2)\n",
    "        )\n",
    "\n",
    "        fitted_alphas.append(\n",
    "            mu_1**2 / (mu_2 - mu_1**2)\n",
    "        )\n",
    "        \n",
    "    bias_alphas.append(np.mean(fitted_alphas) - alpha)\n",
    "    bias_betas.append(np.mean(fitted_betas) - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Bias of alpha estimate vs sample size\")\n",
    "plt.xlabel(\"1 / n\")\n",
    "plt.ylabel(\"Bias of alpha estimate\")\n",
    "plt.plot(1 / sizes, bias_alphas);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Bias of beta estimate vs sample size\")\n",
    "plt.xlabel(\"1 / n\")\n",
    "plt.ylabel(\"Bias of beta estimate\")\n",
    "plt.plot(1 / sizes, bias_betas);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "Что мы можем сказать, сравнив метод моментов и MLE оценки? Как они соотносятся?\n",
    "\n",
    "Когда использовать MoM?\n",
    "\n",
    "1. Для быстрых оценок параметров когда MLE не даёт аналитического решения.\n",
    "2. Для инициализации переменных для более адвансовых методов оптимизации.\n",
    "3. Для оценки казуальных эффектов (область в развитии):\n",
    "    1. https://papers.nips.cc/paper/2019/file/15d185eaa7c954e77f5343d941e25fbd-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что вы подбросили моменту один раз и у вас выпал орёл. \n",
    "\n",
    "Распределение описывается распредлением Бернулли: $p(x=1, q) = q$.\n",
    "\n",
    "Для выборки $x_1, x_2, \\dots, x_N$ правдоподобие записывается следующим образом:\n",
    "\n",
    "$$p(X| q) = \\prod q^{x_i} (1 - q)^{1-x_i}$$\n",
    "\n",
    "$$\\log p(X| q) = \\sum\\left[ x_i \\log q + (1 - x_i) \\log(1 - q) \\right]$$\n",
    "\n",
    "Тогда оценка на параметры высчитывается следующим образом:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial }{\\partial q} \\log p(X| q) = \\frac{1}{q} \\sum x_i - \\frac{1}{1-q} \\sum (1 - x_i) = 0$$\n",
    "\n",
    "Получаем:\n",
    "\n",
    "$$q = \\frac{\\sum x_i}{n}$$\n",
    "\n",
    "В согласии с методом максимума правдоподобия следует, что $p=1$, т.е. следует что монетка всегда будет выпадать орлом.\n",
    "\n",
    "Такая оценка не очень хорошо согласуется с реальностью. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у вас есть некоторое априорное знание. Вы точно знаете, что честная монетка выпадает орлом в 50% случаев.\n",
    "\n",
    "Можем ли мы как-то это использовать? Да, это обеспечивается введением априорного распределения.\n",
    "\n",
    "В MLE параметры оценивались путём максимизации правдоподобия:\n",
    "\n",
    "$$q = \\mathrm{argmax} \\log P(X | q) = \\mathrm{argmax} \\sum \\log p(x_i, q) $$\n",
    "\n",
    "\n",
    "В MAP мы вводим распределение $P(q)$ -- наше априорное знание о распределении параметров. И в MAP мы максимизируем апостериорную вероятность $p(q | X)$:\n",
    "\n",
    "$$\n",
    "q = \\mathrm{argmax}  P(X | q) P(q) = \\mathrm{argmax} \\left( \\sum  \\log p(x_i, q) \\right) P(q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к монете.\n",
    "\n",
    "В качестве априорного распределения выберем бета-распределение со следующей плотностью:\n",
    "\n",
    "$$P(q | \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} q^{\\alpha - 1} (1 - q)^{\\beta - 1}$$\n",
    "\n",
    "![beta](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Beta_distribution_pdf.svg/531px-Beta_distribution_pdf.svg.png)\n",
    "\n",
    "В таком случае апостериорная плотность вероятности выглядит следующим образом:\n",
    "\n",
    "$$p(X, q) p(q) = \\prod q^{x_i} (1 - q)^{1-x_i} \\frac{1}{B(\\alpha, \\beta)} q^{\\alpha - 1} (1 - q)^{\\beta - 1}$$\n",
    "\n",
    "Лог-вероятность:\n",
    "\n",
    "$$\\log p(X, q) p(q) = (\\alpha - 1) \\log q + (\\beta - 1) \\log (1 - q) +  \\sum\\left[ x_i \\log q + (1 - x_i) \\log( - q) \\right] $$\n",
    "\n",
    "Прооптимизируем наше выражение путём приравнивания производной к 0:\n",
    "\n",
    "$$\\frac{\\partial }{\\partial q} \\log p(X, q) p(q) = $$\n",
    "\n",
    "$$ = \\frac{1}{q} \\sum x_i - \\frac{1}{1-q} \\sum (1 - x_i) + \\frac{\\alpha - 1}{q} - \\frac{\\beta - 1}{1 - q} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решая уравнение выше получаем:\n",
    "\n",
    "$$\\mu = \\frac{\\sum x_i + \\alpha - 1}{n + \\beta + \\alpha - 2}$$\n",
    "\n",
    "\n",
    "Пусть наш приор $\\alpha=\\beta=2$ (см. картинку выше).\n",
    "\n",
    "Тогда при одном броске монеты получаем следующую оценку:\n",
    "\n",
    "$$q = \\frac{1 + 2 - 1}{1 + 2 + 2 - 2} = \\frac{2}{3} \\approx 0.66$$\n",
    "\n",
    "\n",
    "### Какой физический смысл у $\\alpha$ и $\\beta$?\n",
    "\n",
    "Вводя прайор $P(q | \\alpha, \\beta)$ мы добавляем $\\alpha - 1$ \"виртуальных\"бросков монеты орлом и $\\beta - 1$ \"виртуальных\" бросков монеты решкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведём небольшой эксперимент с $q = 0.4$ и сравненим  MLE и MAP с разными прайорами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.4\n",
    "coin_flips = np.random.binomial(n=1, p=q, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=100)\n",
    "\n",
    "a = 18\n",
    "b = 42\n",
    "plt.plot((np.cumsum(coin_flips) + a - 1) / (np.cumsum(np.ones_like(coin_flips)) + b + a - 2), label=\"a={}, b={}\".format(a, b))\n",
    "\n",
    "a = 36\n",
    "b = 84\n",
    "plt.plot((np.cumsum(coin_flips) + a - 1) / (np.cumsum(np.ones_like(coin_flips)) + b + a - 2), label=\"a={}, b={}\".format(a, b))\n",
    "\n",
    "a = 24\n",
    "b = 36\n",
    "plt.plot((np.cumsum(coin_flips) + a - 1) / (np.cumsum(np.ones_like(coin_flips)) + b + a - 2), label=\"a={}, b={}\".format(a, b))\n",
    "\n",
    "a = 48\n",
    "b = 72\n",
    "plt.plot((np.cumsum(coin_flips) + a - 1) / (np.cumsum(np.ones_like(coin_flips)) + b + a - 2), label=\"a={}, b={}\".format(a, b))\n",
    "\n",
    "plt.plot(np.cumsum(coin_flips) / np.cumsum(np.ones_like(coin_flips)), label=\"MLE\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "\n",
    "    1. В случае большого размера выборки MLE лучше чем плохой прайор.\n",
    "    2. В случае очень маленького размера выборки (<10) даже плохой прайор лучше чем никакой.\n",
    "    3. В случае среднего размера выборки нет особой разницы между MLE и плохим прайором.\n",
    "    4. Во всех случаях хороший прайор лучше всех (но как его найти?..)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разница (одна из) между байесовским подходом и частотным\n",
    "\n",
    "Для байесовского анализа предположим что прайор у нас плоский, т.е. $p(q) = 1$\n",
    "\n",
    "А теперь давайте сделаем замену координат!\n",
    "\n",
    "$$q = t^2$$\n",
    "\n",
    "Что тогда поменяется?\n",
    "\n",
    "### MLE\n",
    "\n",
    "$$p(X | q) = \\prod q^{x_i} (1 - q)^{1-x_i} \\Rightarrow p(X, t) = \\prod (t^2)^{x_i} (1 - t^2)^{1-x_i}$$\n",
    "\n",
    "\n",
    "Ничего не меняется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP\n",
    "\n",
    "\n",
    "$$p(X | q) = \\prod q^{x_i} (1 - q)^{1-x_i} \\Rightarrow p(X, t) = \\prod (t^2)^{x_i} (1 - t^2)^{1-x_i}$$\n",
    "\n",
    "$$p(q) = 1 \\Rightarrow p(t) = 1$$\n",
    "\n",
    "Тоже ничего не меняется?! Что я сделал не так?\n",
    "\n",
    "\n",
    "### Что же я сделал не так?\n",
    "\n",
    "Это полуфилософский-полуматематический вопрос.\n",
    "\n",
    "В MLE мы максимизируем $p(X|q)$, а $p(X|q)$ это функция от $q$, поэтому замену $q = t^2$ можно сделать без задней мысли и верно следующее:\n",
    "\n",
    "$$q_{MLE} = (t_{MLE})^2$$\n",
    "\n",
    "\n",
    "\n",
    "В MAP мы максимизируем $p(X|q) p(q)$ и вот теперь возникает тонкость!\n",
    "\n",
    "В $p(X|q)$ замену можно сделать без задней мысли опять: $p(X|q) = p(X|t^2)$.\n",
    "\n",
    "Но в $p(q)$ сделать замену напрямую нельзя, потому что $p(q)$ это плотность вероятности, которая __не несёт физического смысла__. Физический смысл есть у вероятности: $p(q) dq = P(q' \\in [q, q+dq])$ и данная вероятность должна быть __инвариантна__ к замене переменных, т.е.\n",
    "\n",
    "$$p(q) dq = p(t^2) dt$$\n",
    "\n",
    "Из чего следует что:\n",
    "\n",
    "$$p(q) = p(t^2) \\frac{dt}{dq}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дававайте проиллюстрируем это на примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.linspace(0, 1, 1000)\n",
    "\n",
    "prior = lambda x: np.ones_like(x) # flat prior\n",
    "transformation = lambda x: x**2 # q = t^2\n",
    "transformation_derivative = lambda x: 2 * x # dq / dt\n",
    "transformation_inverse = lambda x: np.sqrt(x) # t = sqrt(q)\n",
    "\n",
    "\"\"\"\n",
    "transformation = lambda x: np.sin(x)\n",
    "transformation_derivative = lambda x: np.cos(x)\n",
    "transformation_inverse = lambda x: np.arcsin(x)\n",
    "\n",
    "transformation = lambda x: x**3\n",
    "transformation_derivative = lambda x: 3 * x**2\n",
    "transformation_inverse = lambda x: np.power(x, 1/3)\n",
    "\n",
    "transformation = lambda x: np.exp(x)\n",
    "transformation_derivative = lambda x: np.exp(x)\n",
    "transformation_inverse = lambda x: np.log(x)\n",
    "\"\"\"\n",
    "\n",
    "t = transformation_inverse(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "\n",
    "plt.plot(q, t, label=\"q=t^2\", color=\"b\")\n",
    "plt.plot(q, prior(q), label=\"p(q)\", color=\"m\")\n",
    "plt.plot(- prior(q) * transformation_derivative(t), t, label=\"p(t)\", color=\"g\")\n",
    "\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"t\")\n",
    "\n",
    "q_segment = np.linspace(0.01, 0.2, 20)\n",
    "\n",
    "plt.fill_between(q_segment, prior(q_segment), alpha=0.3, color=\"m\")\n",
    "plt.fill_betweenx(transformation_inverse(q_segment), -transformation_derivative(transformation_inverse(q_segment)), alpha=0.3, color='green')\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    [0, q_segment[0]], \n",
    "    [transformation_inverse(q_segment[0]), transformation_inverse(q_segment[0])], \n",
    "    color=\"k\", linestyle='--'\n",
    ")\n",
    "plt.plot(\n",
    "    [0, q_segment[-1]], \n",
    "    [transformation_inverse(q_segment[-1]), transformation_inverse(q_segment[-1])], \n",
    "    color=\"k\", linestyle='--'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    [q_segment[0], q_segment[0]], \n",
    "    [prior(q_segment[0]), transformation_inverse(q_segment[0])], \n",
    "    color=\"k\", linestyle='--'\n",
    ")\n",
    "plt.plot(\n",
    "    [q_segment[-1], q_segment[-1]], \n",
    "    [prior(q_segment[-1]), transformation_inverse(q_segment[-1])], \n",
    "    color=\"k\", linestyle='--'\n",
    ")\n",
    "\n",
    "plt.axvline(0, linewidth=1, color='k', alpha=1)\n",
    "plt.axhline(0, linewidth=1, color='k', alpha=1)\n",
    "\n",
    "xticts = plt.xticks()[0]\n",
    "plt.xticks(xticts, np.abs(xticts))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "    1. MLE инвариантно к замене переменных, то есть вне зависимости от параметризации ответ будет одинаковым\n",
    "    2. MAP не инвариантно к замене переменных, то есть параметризация будет существенно влиять на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
